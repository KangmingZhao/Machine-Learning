{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec40909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class Relu:\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(self.x, 0)\n",
    "\n",
    "    def backward(self, eta):\n",
    "        eta[self.x <= 0] = 0\n",
    "        return eta\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parameter:\n",
    "    def __init__(self, w):\n",
    "        self.data = w     # 权重\n",
    "        self.grad = None  # 传到下一层的梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e012a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, num_channels, moving_decay=0.9, is_training=True):\n",
    "        \"\"\"\n",
    "        初始化BatchNormalization层。\n",
    "\n",
    "        :param num_channels: 输入需要归一化的通道数。\n",
    "        :param moving_decay: 移动平均的衰减率。\n",
    "        :param is_training: 当前是否为训练状态。\n",
    "        \"\"\"\n",
    "        self.gamma = parameter(np.ones((num_channels, 1, 1)))\n",
    "        self.beta = parameter(np.zeros((num_channels, 1, 1)))\n",
    "        self.is_training = is_training\n",
    "        self.epsilon = 1e-5     # 数据归一化时防止下溢\n",
    "\n",
    "        self.moving_mean = np.zeros((num_channels, 1, 1))\n",
    "        self.moving_var = np.zeros((num_channels, 1, 1))\n",
    "        self.moving_decay = moving_decay\n",
    "\n",
    "    def forward(self, x, is_training=True):\n",
    "        \"\"\"\n",
    "        BatchNormalization层的前向传播。\n",
    "\n",
    "        :param x: 输入的 feature map，形状为[N, C, H, W]。\n",
    "        :return: Batch Normalization 的结果，形状为[N, C, H, W]。\n",
    "        \"\"\"\n",
    "        self.is_training = is_training\n",
    "        N, C, H, W = x.shape\n",
    "        self.x = x\n",
    "\n",
    "        if self.is_training:\n",
    "            # 计算均值和方差\n",
    "            mean = np.mean(x, axis=(0, 2, 3))[:, np.newaxis, np.newaxis]\n",
    "            var = np.var(x, axis=(0, 2, 3))[:, np.newaxis, np.newaxis]\n",
    "\n",
    "            # 计算滑动平均\n",
    "            if np.sum(self.moving_mean) == 0 and np.sum(self.moving_var) == 0:\n",
    "                self.moving_mean = mean\n",
    "                self.moving_var = var\n",
    "            else:\n",
    "                self.moving_mean = self.moving_mean * self.moving_decay + (1 - self.moving_decay) * mean\n",
    "                self.moving_var = self.moving_var * self.moving_decay + (1 - self.moving_decay) * var\n",
    "\n",
    "            # 归一化\n",
    "            self.y = (x - mean) / np.sqrt(var + self.epsilon)\n",
    "            return self.gamma.data * self.y + self.beta.data\n",
    "        else:\n",
    "            # 测试阶段使用移动平均的均值和方差进行标准化\n",
    "            self.y = (x - self.moving_mean) / np.sqrt(self.moving_var + self.epsilon)\n",
    "            return self.gamma.data * self.y + self.beta.data\n",
    "\n",
    "    def backward(self, eta, learning_rate):\n",
    "        \"\"\"\n",
    "        BatchNormalization层的反向传播。\n",
    "\n",
    "        :param eta: 上一层传回的梯度，形状为[N, C, H, W]。\n",
    "        :param learning_rate: 学习率。\n",
    "        :return: 传到上一层的梯度，形状为[N, C, H, W]。\n",
    "        \"\"\"\n",
    "        # 计算 gamma 和 beta 的梯度\n",
    "        N, _, H, W = eta.shape\n",
    "        gamma_grad = np.sum(eta * self.y, axis=(0, 2, 3))\n",
    "        beta_grad = np.sum(eta, axis=(0, 2, 3))\n",
    "\n",
    "        # 返回到上一层的梯度\n",
    "        yx_grad = eta * self.gamma.data\n",
    "        ymean_grad = (-1.0 / np.sqrt(self.var + self.epsilon)) * yx_grad\n",
    "        ymean_grad = np.sum(ymean_grad, axis=(2, 3))[:, :, np.newaxis, np.newaxis] / (H * W)\n",
    "        yvar_grad = -0.5 * yx_grad * (self.x - self.mean) / (self.var + self.epsilon)**(3.0/2)\n",
    "        yvar_grad = 2 * (self.x - self.mean) * np.sum(yvar_grad, axis=(2, 3))[:, :, np.newaxis, np.newaxis] / (H * W)\n",
    "        result = yx_grad * (1 / np.sqrt(self.var + self.epsilon)) + ymean_grad + yvar_grad\n",
    "\n",
    "        # 更新 gamma 和 beta\n",
    "        self.gamma.data -= learning_rate * gamma_grad[:, np.newaxis, np.newaxis] / N\n",
    "        self.beta.data -= learning_rate * beta_grad[:, np.newaxis, np.newaxis] / N\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a3f82",
   "metadata": {},
   "source": [
    "### 卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118426f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv:\n",
    "    def __init__(self, filter_shape, stride=1, padding='SAME', bias=True, requires_grad=True):\n",
    "        \"\"\"\n",
    "        初始化卷积层。\n",
    "\n",
    "        :param filter_shape: 元组 (O, C, K, K)，表示输出通道数、输入通道数和卷积核大小。\n",
    "        :param stride: 卷积操作的步幅。\n",
    "        :param padding: 填充类型：{\"SAME\", \"VALID\"}。\n",
    "        :param bias: 是否包含偏置。\n",
    "        :param requires_grad: 是否在反向传播中计算梯度。\n",
    "        \"\"\"\n",
    "        # 使用 Kaiming 初始化权重\n",
    "        self.weight = parameter(np.random.randn(*filter_shape) * (2/reduce(lambda x, y: x*y, filter_shape[1:]))**0.5)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.requires_grad = requires_grad\n",
    "        self.output_channel = filter_shape[0]\n",
    "        self.input_channel = filter_shape[1]\n",
    "        self.filter_size = filter_shape[2]\n",
    "\n",
    "        # 如果启用偏置，则初始化偏置\n",
    "        if bias:\n",
    "            self.bias = parameter(np.random.randn(self.output_channel))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        执行卷积层的前向传播。\n",
    "\n",
    "        :param input: 形状为 [N, C, H, W] 的特征图。\n",
    "        :return: 卷积输出，形状为 [N, O, output_H, output_W]。\n",
    "        \"\"\"\n",
    "        # 第一步：应用填充\n",
    "        if self.padding == \"VALID\":\n",
    "            self.x = input\n",
    "        if self.padding == \"SAME\":\n",
    "            p = self.filter_size // 2\n",
    "            self.x = np.lib.pad(input, ((0, 0), (0, 0), (p, p), (p, p)), \"constant\")\n",
    "\n",
    "        # 第二步：调整输入维度以适应步幅\n",
    "        self.adjust_input_dimensions()\n",
    "\n",
    "        # 第三步：实现卷积\n",
    "        N, _, H, W = self.x.shape\n",
    "        O, C, K, K = self.weight.data.shape\n",
    "        weight_cols = self.weight.data.reshape(O, -1).T\n",
    "        x_cols = self.img2col(self.x, self.filter_size, self.filter_size, self.stride)\n",
    "        result = np.dot(x_cols, weight_cols) + self.bias.data\n",
    "        output_H, output_W = (H-self.filter_size)//self.stride + 1, (W-self.filter_size)//self.stride + 1\n",
    "        result = result.reshape((N, result.shape[0]//N, -1)).reshape((N, output_H, output_W, O))\n",
    "        return result.transpose((0, 3, 1, 2))\n",
    "\n",
    "    def backward(self, eta, lr):\n",
    "        \"\"\"\n",
    "        反向传播，更新权重和计算传递到上一层的梯度。\n",
    "\n",
    "        :param eta: 上一层返回的梯度 [N, O, output_H, output_W]。\n",
    "        :param lr: 学习率。\n",
    "        :return: 上一层的梯度。\n",
    "        \"\"\"\n",
    "        # 在eta的行和列之间插入零，处理步长大于1的情况\n",
    "        self.insert_zeros_in_eta(eta)\n",
    "\n",
    "        # 计算本层的权重和偏置的梯度\n",
    "        N, _, output_H, output_W = eta.shape\n",
    "        self.calculate_gradients(eta, N, output_H, output_W)\n",
    "\n",
    "        # 更新权重和偏置\n",
    "        self.update_weights(lr, N)\n",
    "\n",
    "        # 第四步：边缘填充\n",
    "        self.pad_eta(eta)\n",
    "\n",
    "        # 计算传递到上一层的梯度\n",
    "        result = self.calculate_gradient_to_prev_layer(eta)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def adjust_input_dimensions(self):\n",
    "        \"\"\"\n",
    "        调整输入特征图的维度以适应卷积步幅。\n",
    "        \"\"\"\n",
    "        if self.stride > 1:\n",
    "            x_fit = self.x.shape[2] % self.stride\n",
    "            y_fit = self.x.shape[3] % self.stride\n",
    "            if x_fit != 0:\n",
    "                self.x = self.x[:, :, 0:self.x.shape[2] - x_fit, :]\n",
    "            if y_fit != 0:\n",
    "                self.x = self.x[:, :, :, 0:self.x.shape[3] - y_fit]\n",
    "\n",
    "    def insert_zeros_in_eta(self, eta):\n",
    "        \"\"\"\n",
    "        在eta的行和列之间插入零，以处理步长大于1的情况。\n",
    "\n",
    "        :param eta: 上一层返回的梯度 [N, O, output_H, output_W]。\n",
    "        \"\"\"\n",
    "        if self.stride > 1:\n",
    "            N, O, output_H, output_W = eta.shape\n",
    "            inserted_H, inserted_W = output_H + (output_H-1)*(self.stride-1), output_W + (output_W-1)*(self.stride-1)\n",
    "            inserted_eta = np.zeros((N, O, inserted_H, inserted_W))\n",
    "            inserted_eta[:,:,::self.stride, ::self.stride] = eta\n",
    "            eta = inserted_eta\n",
    "\n",
    "    def calculate_gradients(self, eta, N, output_H, output_W):\n",
    "        \"\"\"\n",
    "        计算本层的权重和偏置的梯度。\n",
    "\n",
    "        :param eta: 上一层返回的梯度 [N, O, output_H, output_W]。\n",
    "        :param N: batch大小。\n",
    "        :param output_H: 输出特征图的高度。\n",
    "        :param output_W: 输出特征图的宽度。\n",
    "        \"\"\"\n",
    "        self.b_grad = eta.sum(axis=(0,2,3))\n",
    "        self.W_grad = np.zeros(self.weight.data.shape)\n",
    "        for i in range(self.filter_size):\n",
    "            for j in range(self.filter_size):\n",
    "                self.W_grad[:,:,i,j] = np.tensordot(eta, self.x[:,:,i:i+output_H,j:j+output_W], ([0,2,3], [0,2,3]))\n",
    "\n",
    "    def update_weights(self, lr, N):\n",
    "        \"\"\"\n",
    "        更新权重和偏置。\n",
    "\n",
    "        :param lr: 学习率。\n",
    "        :param N: batch大小。\n",
    "        \"\"\"\n",
    "        self.weight.data -= lr * self.W_grad / N\n",
    "        if self.bias is not None:\n",
    "            self.bias.data -= lr * self.b_grad / N\n",
    "\n",
    "    def pad_eta(self, eta):\n",
    "        \"\"\"\n",
    "        边缘填充eta，处理填充类型为\"VALID\"或\"SAME\"的情况。\n",
    "\n",
    "        :param eta: 上一层返回的梯度 [N, O, output_H, output_W]。\n",
    "        \"\"\"\n",
    "        if self.padding == \"VALID\":\n",
    "            p = self.filter_size - 1\n",
    "            pad_eta = np.lib.pad(eta, ((0,0),(0,0),(p,p),(p,p)), \"constant\", constant_values=0)\n",
    "            eta = pad_eta\n",
    "        elif self.padding == \"SAME\":\n",
    "            p = self.filter_size // 2\n",
    "            pad_eta = np.lib.pad(eta, ((0, 0), (0, 0), (p, p), (p, p)), \"constant\", constant_values=0)\n",
    "            eta = pad_eta\n",
    "\n",
    "    def calculate_gradient_to_prev_layer(self, eta):\n",
    "        \"\"\"\n",
    "        计算传递到上一层的梯度。\n",
    "\n",
    "        :param eta: 上一层返回的梯度 [N, O, output_H, output_W]。\n",
    "        :return: 传递到上一层的梯度。\n",
    "        \"\"\"\n",
    "        _, C, _, _ = self.weight.data.shape\n",
    "        weight_flip = np.flip(self.weight.data, (2,3))\n",
    "        weight_flip_swap = np.swapaxes(weight_flip, 0, 1)\n",
    "        weight_flip = weight_flip_swap.reshape(C, -1).T\n",
    "        x_cols = self.img2col(eta, self.filter_size, self.filter_size, self.stride)\n",
    "        result = np.dot(x_cols, weight_flip)\n",
    "        N, _, H, W = eta.shape\n",
    "        output_H, output_W = (H - self.filter_size) // self.stride + 1, (W - self.filter_size) // self.stride + 1\n",
    "        result = result.reshape((N, result.shape[0] // N, -1)).reshape((N, output_H, output_W, C))\n",
    "        self.weight.grad = result.transpose((0, 3, 1, 2))\n",
    "        return self.weight.grad\n",
    "\n",
    "    def img2col(self, x, filter_size_x, filter_size_y, stride):\n",
    "        \"\"\"\n",
    "        将输入特征图转换为二维矩阵。\n",
    "\n",
    "        :param x: 输入特征图，形状为 [N, C, H, W]。\n",
    "        :param filter_size_x: 卷积核的尺寸x。\n",
    "        :param filter_size_y: 卷积核的尺寸y。\n",
    "        :param stride: 卷积步长。\n",
    "        :return: 二维矩阵，形状为 [(H-filter_size+1)/stride * (W-filter_size+1)/stride*N, C * filter_size_x * filter_size_y]。\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "        output_H, output_W = (H-filter_size_x)//stride + 1, (W-filter_size_y)//stride + 1\n",
    "        out_size = output_H * output_W\n",
    "        x_cols = np.zeros((out_size*N, filter_size_x*filter_size_y*C))\n",
    "        for i in range(0, H-filter_size_x+1, stride):\n",
    "            i_start = i * output_W\n",
    "            for j in range(0, W-filter_size_y+1, stride):\n",
    "                temp = x[:,:, i:i+filter_size_x, j:j+filter_size_y].reshape(N,-1)\n",
    "                x_cols[i_start+j::out_size, :] = temp\n",
    "        return x_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c40a29",
   "metadata": {},
   "source": [
    "### 池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 池化层 分为 平均池化和最大池化\n",
    "class MaxPooling:\n",
    "    def __init__(self, kernel_size=(2, 2), stride=2):\n",
    "        \"\"\"\n",
    "        :param kernel_size: 池化核的大小(kx, ky)\n",
    "        :param stride: 步长\n",
    "        这里有个默认的前提条件就是：kernel_size=stride\n",
    "        \"\"\"\n",
    "        self.ksize = kernel_size\n",
    "        self.stride = stride\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        :param input: feature map形状[N, C, H, W]\n",
    "        :return: maxpooling后的结果[N, C, H//ksize, W//ksize]\n",
    "        \"\"\"\n",
    "        N, C, H, W = input.shape\n",
    "        input_grid = input.reshape(N, C, H // self.stride, self.stride, W // self.stride, self.stride)\n",
    "        out = np.max(input_grid, axis=(3, 5))\n",
    "        self.mask = (out.repeat(self.stride, axis=2).repeat(self.stride, axis=3) != input)\n",
    "        return out\n",
    "\n",
    "    def backward(self, eta):\n",
    "        \"\"\"\n",
    "        :param eta: 上一层返回的梯度[N, C, H//ksize, W//ksize]\n",
    "        :return: 反向传播的梯度[N, C, H, W]\n",
    "        \"\"\"\n",
    "        result = eta.repeat(self.stride, axis=2).repeat(self.stride, axis=3)\n",
    "        result[self.mask] = 0\n",
    "        return result\n",
    "\n",
    "\n",
    "class Averagepooling():\n",
    "    def __init__(self, kernel_size=(2,2), stride=2):\n",
    "        \"\"\"\n",
    "        :param kernel_size:池化核的大小(kx,ky)\n",
    "        :param stride: 步长\n",
    "        \"\"\"\n",
    "        self.ksize = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        out = input.reshape((N, C, H//self.ksize, self.ksize, W//self.ksize, self.ksize))\n",
    "        out = out.sum(axis=(3,5))\n",
    "        out = out / self.ksize**2\n",
    "        return out\n",
    "\n",
    "    def backward(self, eta):\n",
    "        result = eta.repeat(self.ksize, axis=2).repeat(self.ksize, axis=3)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14e7a1",
   "metadata": {},
   "source": [
    "### 全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dba3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc:\n",
    "    def __init__(self, input_num, output_num, bias=True, requires_grad=True):\n",
    "        \"\"\"\n",
    "        :param input_num:输入神经元个数\n",
    "        :param output_num: 输出神经元的个数\n",
    "        \"\"\"\n",
    "        self.input_num = input_num          # 输入神经元个数\n",
    "        self.output_num = output_num        # 输出神经元个数\n",
    "        self.requires_grad = requires_grad\n",
    "        self.weight = parameter(np.random.randn(self.input_num, self.output_num) * (2/self.input_num**0.5))\n",
    "        if bias:\n",
    "            self.bias = parameter(np.random.randn(self.output_num))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        :param input: 输入的feature map 形状：[N,C,H,W]或[N,C*H*W]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.input_shape = input.shape    # 记录输入数据的形状\n",
    "        if input.ndim > 2:\n",
    "            N, C, H, W = input.shape\n",
    "            self.x = input.reshape((N, -1))\n",
    "        elif input.ndim == 2:\n",
    "            self.x = input\n",
    "        else:\n",
    "            print(\"fc.forward的输入数据维度存在问题\")\n",
    "        result = np.dot(self.x, self.weight.data)\n",
    "        if self.bias is not None:\n",
    "            result = result + self.bias.data\n",
    "        return result\n",
    "\n",
    "\n",
    "    def backward(self, eta, lr):\n",
    "        \"\"\"\n",
    "        :param eta:由上一层传入的梯度 形状：[N,output_num]\n",
    "        :param lr:学习率\n",
    "        :return: self.weight.grad 回传到上一层的梯度\n",
    "        \"\"\"\n",
    "        N, _ = eta.shape\n",
    "        # 计算传到下一层的梯度\n",
    "        next_eta = np.dot(eta, self.weight.data.T)\n",
    "        self.weight.grad = np.reshape(next_eta, self.input_shape)\n",
    "\n",
    "        # 计算本层W,b的梯度\n",
    "        x = self.x.repeat(self.output_num, axis=0).reshape((N, self.output_num, -1))\n",
    "        self.W_grad = x * eta.reshape((N, -1, 1))\n",
    "        self.W_grad = np.sum(self.W_grad, axis=0) / N\n",
    "        self.b_grad = np.sum(eta, axis=0) / N\n",
    "\n",
    "        # 权重更新\n",
    "        self.weight.data -= lr * self.W_grad.T\n",
    "        self.bias.data -= lr * self.b_grad\n",
    "\n",
    "        return self.weight.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3971a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout():\n",
    "    def __init__(self, drop_rate=0.5, is_train=True):\n",
    "        \"\"\"\n",
    "        :param drop_rate: 随机丢弃神经元的概率\n",
    "        :param is_train: 当前是否为训练状态\n",
    "        \"\"\"\n",
    "        self.drop_rate = drop_rate\n",
    "        self.is_train = is_train\n",
    "        self.fix_value = 1 - drop_rate   # 修正期望，保证输出值的期望不变\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x:[N, m] N为batch_size, m为神经元个数\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.is_train==False:    # 当前为测试状态\n",
    "            return x\n",
    "        else:             # 当前为训练状态\n",
    "            N, m = x.shape\n",
    "            self.save_mask = np.random.uniform(0, 1, m) > self.drop_rate   # save_mask中为保留的神经元\n",
    "            return (x * self.save_mask) / self.fix_value\n",
    "\n",
    "\n",
    "    def backward(self, eta):\n",
    "        if self.is_train==False:\n",
    "            return eta\n",
    "        else:\n",
    "            return eta * self.save_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6941f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax 计算损失函数\n",
    "class softmax:\n",
    "\n",
    "    def calculate_loss(self, x, label):\n",
    "        \"\"\"\n",
    "        :param x: 上一层输出的向量：[N, m] 其中N表示batch，m表示输出节点个数\n",
    "        :param label:数据的真实标签：[N]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        N, _ = x.shape\n",
    "        self.label = np.zeros_like(x)\n",
    "        for i in range(self.label.shape[0]):\n",
    "            self.label[i, label[i]] = 1\n",
    "\n",
    "        self.x = np.exp(x - np.max(x, axis=1)[:, np.newaxis])   # 为了防止x中出现极值导致溢出，每个样本减去其中最大的值\n",
    "        sum_x = np.sum(self.x, axis=1)[:, np.newaxis]\n",
    "        self.prediction = self.x / sum_x\n",
    "\n",
    "        self.loss = -np.sum(np.log(self.prediction+1e-6) * self.label)  # 防止出现log(0)的情况\n",
    "        return self.loss / N\n",
    "\n",
    "    def prediction_func(self, x):\n",
    "        x = np.exp(x - np.max(x, axis=1)[:, np.newaxis])  # 为了防止x中出现极值导致溢出，每个样本减去其中最大的值\n",
    "        sum_x = np.sum(x, axis=1)[:, np.newaxis]\n",
    "        self.out = x / sum_x\n",
    "        return self.out\n",
    "\n",
    "\n",
    "    def gradient(self):\n",
    "        self.eta = self.prediction.copy() - self.label\n",
    "        return self.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lenet5\n",
    "class LeNet5:\n",
    "\n",
    "    def __init__(self):\n",
    "        ## 第一层卷积 输入通道数 1 输出通道数 6 卷积核5*5\n",
    "        self.conv1 = conv((6, 1, 5, 5), stride=1, padding='SAME', bias=True, requires_grad=True)\n",
    "        self.pooling1 = MaxPooling(kernel_size=(2, 2), stride=2)\n",
    "        #self.pooling1 = Averagepooling(kernel_size=(2, 2), stride=2)\n",
    "        self.BN1 =BatchNormalization(6, moving_decay=0.95, is_train=True)\n",
    "        self.relu1 = Relu()\n",
    "         ## 第一层卷积 输入通道数 6 输出通道数 16 卷积核5*5\n",
    "        self.conv2 = conv((16, 6, 5, 5), stride=1, padding=\"VALID\", bias=True, requires_grad=True)\n",
    "        self.pooling2=MaxPooling(kernel_size=(2,2),stride=2)\n",
    "        #self.pooling2 = Averagepooling(kernel_size=(2, 2), stride=2)\n",
    "        self.BN2 = BatchNormalization(16, moving_decay=0.95, is_train=True)\n",
    "        self.relu2 = Relu()\n",
    "         ## 第一层卷积 输入通道数 16 输出通道数 120 卷积核5*5\n",
    "        self.conv3 = conv((120, 16, 5, 5), stride=1, padding=\"VALID\", bias=True, requires_grad=True)\n",
    "        self.fc4 = fc(120*1*1, 84, bias=True, requires_grad=True)\n",
    "        self.relu4 = Relu()\n",
    "        self.dropout=Dropout(drop_rate=0.5,is_train=True)\n",
    "        self.fc5 = fc(84, 10, bias=True, requires_grad=True)\n",
    "\n",
    "        self.softmax = softmax()\n",
    "\n",
    "    def forward(self, imgs, labels, is_train=True):\n",
    "        \"\"\"\n",
    "        :param imgs:输入的图片：[N,C,H,W]\n",
    "        :param labels:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = self.conv1.forward(imgs)\n",
    "        x = self.pooling1.forward(x)\n",
    "        x = self.BN1.forward(x, is_train)\n",
    "        x = self.relu1.forward(x)\n",
    "\n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.pooling2.forward(x)\n",
    "        x = self.BN2.forward(x, is_train)\n",
    "        x = self.relu2.forward(x)\n",
    "\n",
    "        x = self.conv3.forward(x)\n",
    "\n",
    "        x = self.fc4.forward(x)\n",
    "        x = self.relu4.forward(x)\n",
    "        x = self.dropout.forward(x)  # 应用Dropout\n",
    "        x = self.fc5.forward(x)\n",
    "\n",
    "        loss = self.softmax.calculate_loss(x, labels)\n",
    "        prediction = self.softmax.prediction_func(x)\n",
    "        return loss, prediction\n",
    "\n",
    "    def backward(self, lr):\n",
    "        \"\"\"\n",
    "        :param lr:学习率\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        eta = self.softmax.gradient()\n",
    "\n",
    "        eta = self.fc5.backward(eta, lr)\n",
    "        eta = self.dropout.forward(eta)  # 应用Dropout\n",
    "        eta = self.relu4.backward(eta)\n",
    "        eta = self.fc4.backward(eta, lr)\n",
    "\n",
    "        eta = self.conv3.backward(eta, lr)\n",
    "\n",
    "        eta = self.relu2.backward(eta)  # 激活层没有参数，不需要学习\n",
    "        eta = self.BN2.backward(eta, lr)\n",
    "        eta = self.pooling2.backward(eta)     # 池化层没有参数，不需要学习\n",
    "        eta = self.conv2.backward(eta, lr)\n",
    "\n",
    "        eta = self.relu1.backward(eta)\n",
    "        eta = self.BN1.backward(eta, lr)\n",
    "        eta = self.pooling1.backward(eta)\n",
    "        eta = self.conv1.backward(eta, lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import LeNet5\n",
    "\n",
    "\n",
    "def load_mnist(mnistdir , train):\n",
    "    ministfile = open(mnistdir,'rb')\n",
    "    ministdata = ministfile.read()\n",
    "    ministfile.close()\n",
    "    rows=1\n",
    "    cols=1\n",
    "    # 加载训练集\n",
    "    if train:\n",
    "        magic_num,images,rows,cols = struct.unpack_from('>iiii', ministdata,0)\n",
    "    else:\n",
    "        # 加载标签集\n",
    "        magic_num,images = struct.unpack_from('>ii', ministdata,0)\n",
    "    print('图片数量: %d张, 图片大小: %d*%d' % ( images, rows, cols))\n",
    "    # 计算加载的总像素是多少\n",
    "    size = images * rows * cols\n",
    "    if train:\n",
    "        pointer = struct.calcsize('>iiii')\n",
    "    else :\n",
    "        pointer =  struct.calcsize('>ii')\n",
    "    pack_data = struct.unpack_from('>' + str(size) + 'B', ministdata,pointer)\n",
    "    if train:\n",
    "        pack_data = np.reshape(pack_data,[images,rows,cols])\n",
    "    else:\n",
    "        pack_data = np.reshape(pack_data,[images])\n",
    "    # 最终返回了一个矩阵，矩阵的大小由是训练集还是标签集决定\n",
    "    # 训练集就相当于返回好多页纸，每一页纸上面有对应的行数和列数\n",
    "    print('本次解析的矩阵格式为[%d,%d,%d]' % (images,rows,cols))\n",
    "    return pack_data\n",
    "\n",
    "\n",
    "train_images = load_mnist(\"data/train-images.idx3-ubyte\",True)\n",
    "train_labels = load_mnist(\"data/train-labels.idx1-ubyte\",False)\n",
    "test_images = load_mnist(\"data/t10k-images.idx3-ubyte\",True)\n",
    "test_labels = load_mnist(\"data/t10k-labels.idx1-ubyte\",False)\n",
    "\n",
    "train_batch = 64  # 训练时的batch size\n",
    "test_batch = 50  # 测试时的batch size\n",
    "epoch = 10\n",
    "lr = 1e-3\n",
    "# 绘图所需变量\n",
    "TrainTimes = []\n",
    "TrainLoss = []\n",
    "TrainAcc = []\n",
    "TestTimes = []\n",
    "TestAcc = []\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "IterNum = 0     # 迭代次数\n",
    "net = LeNet5.LeNet5()\n",
    "\n",
    "for E in range(epoch):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    ###\n",
    "    # 训练\n",
    "    for i in range(train_images.shape[0] // train_batch):\n",
    "        img = train_images[i * train_batch:(i + 1) * train_batch].reshape(train_batch, 1, 28, 28)\n",
    "        label = train_labels[i * train_batch:(i + 1) * train_batch]\n",
    "        loss, pred = net.forward(img, label, is_train=True)   # 训练阶段\n",
    "\n",
    "        epoch_loss += loss\n",
    "        batch_loss += loss\n",
    "        for j in range(pred.shape[0]):\n",
    "            if np.argmax(pred[j]) == label[j]:\n",
    "                epoch_acc += 1\n",
    "                batch_acc += 1\n",
    "\n",
    "        net.backward(lr)\n",
    "\n",
    "        ###\n",
    "        # 日志输出、图像绘制\n",
    "        print_size = 10\n",
    "        if (i+1) % print_size == 0:\n",
    "            print(f\"Epoch{E+1}:\\tbatch:{i+1}\\tBatch acc:{batch_acc/(train_batch * print_size):{6}.{4}}\\tBatch loss:{batch_loss/(train_batch * print_size):{6}.{4}}\")\n",
    "            IterNum += 1\n",
    "            TrainTimes.append(IterNum)\n",
    "            TrainLoss.append(batch_loss / (train_batch * 10))\n",
    "            TrainAcc.append(batch_acc / (train_batch * 10))\n",
    "            batch_loss = 0\n",
    "            batch_acc = 0\n",
    "        ###\n",
    "    ###\n",
    "    print(f\"[Epoch{E+1}]\\tTarin accuracy:{epoch_acc/train_images.shape[0]:.{4}}\\tTarin loss:{epoch_loss/train_images.shape[0]:.{4}}\")\n",
    "    ###\n",
    "    # 测试集\n",
    "    test_acc = 0\n",
    "    for k in range(test_images.shape[0] // test_batch):\n",
    "        img = test_images[k*test_batch:(k+1)*test_batch].reshape(test_batch, 1, 28, 28)\n",
    "        label = test_labels[k*test_batch:(k+1)*test_batch]\n",
    "        _, pred = net.forward(img, label, is_train=False)\n",
    "\n",
    "        for j in range(pred.shape[0]):\n",
    "            if np.argmax(pred[j]) == label[j]:\n",
    "                test_acc += 1\n",
    "\n",
    "    print(f\"[Epoch{E+1}]\\tTest Accuracy:{test_acc / test_images.shape[0]:.{4}}\")\n",
    "    print(\"-------------------Epoch end---------------------------\")\n",
    "    TestTimes.append(E+1)\n",
    "    TestAcc.append(test_acc / test_images.shape[0])\n",
    "\n",
    "# 绘制图表\n",
    "plt.ioff()\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 训练集损失和准确率\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Training Metrics')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Value')\n",
    "plt.plot(TrainTimes, TrainLoss, label='Loss')\n",
    "plt.plot(TrainTimes, TrainAcc, label='Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 测试集准确率\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(TestTimes, TestAcc, label='Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
